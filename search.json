[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started with LatinCy",
    "section": "",
    "text": "Preface\n‚ÄúGetting Started with LatinCy‚Äù is an always-a-work-in-progress combination of documentation and demo notebooks for working with the LatinCy models on a variety of Latin text analysis and NLP tasks."
  },
  {
    "objectID": "index.html#key-links",
    "href": "index.html#key-links",
    "title": "Getting Started with LatinCy",
    "section": "Key links",
    "text": "Key links\nüì¶Models: https://huggingface.co/latincy\nüååUniverse: https://spacy.io/universe/project/latincy\nüìùPreprint: https://arxiv.org/abs/2305.04365\nThis book has been written using Jupyter notebooks which have then been collated with Quarto. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "abbreviations.html",
    "href": "abbreviations.html",
    "title": "Abbreviations",
    "section": "",
    "text": "Where possible, I will include references to standard NLP works using the following abbreviations:\nSLP Jurafsky, D., and Martin, J.H. 2020. Speech and Language Processing. 3rd Edition, Draft. https://web.stanford.edu/~jurafsky/slp3/. (Jurafsky and Martin 2020)\n\n\n\n\nJurafsky, Daniel, and James H. Martin. 2020. ‚ÄúSpeech and Language Processing (3rd Edition, Draft).‚Äù https://web.stanford.edu/~jurafsky/slp3/."
  },
  {
    "objectID": "2_install.html#installing-spacy",
    "href": "2_install.html#installing-spacy",
    "title": "1¬† Installing LatinCy models",
    "section": "1.1 Installing spaCy",
    "text": "1.1 Installing spaCy\nThe LatinCy models are designed to work with the spaCy natural language platform, so you will need to have this package installed before anything else. The following cell has the pip install command for spaCy. At the time of writing, the latest version available for spaCy compatible with LatinCy is v3.7.2.\nNB: To run the cells below, uncomment the commands by removing the # at the beginning of the line. The exclamation point at the beginning of the line is shorthand for the %system magic command in Jupyter which can be used to run shell commands from within a notebook.\n\n# !pip install spacy==3.7.2"
  },
  {
    "objectID": "2_install.html#installing-the-latincy-models",
    "href": "2_install.html#installing-the-latincy-models",
    "title": "1¬† Installing LatinCy models",
    "section": "1.2 Installing the LatinCy models",
    "text": "1.2 Installing the LatinCy models\nLatinCy models are currently available in three sizes: ‚Äòsm‚Äô, ‚Äòmd‚Äô, and ‚Äòlg‚Äô. We will use the different models throughout the tutorials, so let‚Äôs install all three now so that they are available for future chapters.\n\n# !pip install https://huggingface.co/latincy/la_core_web_sm/resolve/main/la_core_web_sm-any-py3-none-any.whl\n# !pip install https://huggingface.co/latincy/la_core_web_md/resolve/main/la_core_web_md-any-py3-none-any.whl\n# !pip install https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl"
  },
  {
    "objectID": "2_install.html#installing-additional-packages",
    "href": "2_install.html#installing-additional-packages",
    "title": "1¬† Installing LatinCy models",
    "section": "1.3 Installing additional packages",
    "text": "1.3 Installing additional packages\nWe will also use other packages throughout these tutorials. They are included here for your convenience, as well as in the requirements.txt file in the code repository for this Quarto book.\n\n# !pip install pandas\n# !pip install matplotlib\n# !pip install seaborn\n# !pip install scikit-learn\n# !pip install tqdm"
  },
  {
    "objectID": "3_load.html#loading-latincy-models-with-spacy",
    "href": "3_load.html#loading-latincy-models-with-spacy",
    "title": "2¬† Loading LatinCy models",
    "section": "2.1 Loading LatinCy models with spaCy",
    "text": "2.1 Loading LatinCy models with spaCy\n\n# Imports\n\nimport spacy\nfrom pprint import pprint\n\n\nnlp = spacy.load('la_core_web_lg')"
  },
  {
    "objectID": "3_load.html#creating-a-spacy-doc-from-a-string",
    "href": "3_load.html#creating-a-spacy-doc-from-a-string",
    "title": "2¬† Loading LatinCy models",
    "section": "2.2 Creating a spaCy doc from a string",
    "text": "2.2 Creating a spaCy doc from a string\n\ntext = \"Haec narrantur a poetis de Perseo.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo.\n\n\n\nprint(type(doc))\n\n&lt;class 'spacy.tokens.doc.Doc'&gt;\n\n\n\nprint(doc.__repr__())\n\nHaec narrantur a poetis de Perseo.\n\n\n\nprint(doc.text)\n\nHaec narrantur a poetis de Perseo.\n\n\n\nprint(type(doc.text))\n\n&lt;class 'str'&gt;"
  },
  {
    "objectID": "sentence-segmentation.html#sentence-segmentation-with-latincy",
    "href": "sentence-segmentation.html#sentence-segmentation-with-latincy",
    "title": "3¬† Sentence segmentation",
    "section": "3.1 Sentence segmentation with LatinCy",
    "text": "3.1 Sentence segmentation with LatinCy\n\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_lg')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\n\n\nSentence segmentation is the task of splitting a text into sentences. For the LatinCy models, this is a task that is jointly learned with dependency parsing and has been trained to terminate sentences at both strong and weak stops, following the example of Clayman (1981) (see also, Wake (1957), Janson (1964)), who writes: ‚ÄúIf all stops are made equivalent, i.e.¬†if no distinction is made between the strong stop, weak stop and interrogation mark, editorial differences will be kept to a minimum.‚Äù\nGiven a spaCy Doc, the sents attribute will produce a generator object with the sentence from that document as determined by the dependency parser. Each sentence is a Span object with the start and end token indices from the original Doc.\n\nsents = doc.sents\nprint(type(sents))\n\n&lt;class 'generator'&gt;\n\n\nLike all Span objects, the text from each sentence can be retrieved with the text attribute. For convenience below, we convert the generator to list so that we can iterate over it multiple times. Here are the three (3) sentences identified in the example text as well as an indication of the sentences‚Äô type, i.e.¬†&lt;class 'spacy.tokens.span.Span'&gt;.\n\nsents = list(sents)\n\nfor i, sent in enumerate(sents, 1):\n    print(f'{i}: {sent.text}')\n\n1: Haec narrantur a poetis de Perseo.\n2: Perseus filius erat Iovis, maximi deorum.\n3: Avus eius Acrisius appellabatur.\n\n\n\nsent = sents[0]\nprint(type(sent))\n\n&lt;class 'spacy.tokens.span.Span'&gt;\n\n\nSentences have the same atrributes/methods available to them as any span (listed in the next cell). Following are some attibutes/methods that may be particularly relevant to working with sentences.\n\nsent_methods = [item for item in dir(sent) if '_' not in item]\npprint(sent_methods)\n\n['conjuncts',\n 'doc',\n 'end',\n 'ents',\n 'id',\n 'label',\n 'lefts',\n 'rights',\n 'root',\n 'sent',\n 'sentiment',\n 'sents',\n 'similarity',\n 'start',\n 'subtree',\n 'tensor',\n 'text',\n 'vector',\n 'vocab']\n\n\nYou can identify the root of the sentence as determined by the dependency parser. Assuming the parsing in correct, this will be the main verb of the sentence.\n\nprint(sent.root)\n\nnarrantur\n\n\nEach word in the sentence has an associated vector. Sentence (any Span in fact) has an associated vector as well that is the mean of the vectors of the words in the sentence. As this example uses the lg model, the vector has a length of 300.\n\nprint(sent.vector.shape)\n\n(300,)\n\n\nThis vector then can be used to compute the similarity between two sentences. Here we see our example sentence compared to two related sentence: 1. a sentence where the character referred to is changed from Perseus to Ulysses; and 2. the active-verb version of the sentence.\n\nsent.similarity(nlp('Haec narrantur a poetis de Ulixe.'))\n\n0.9814934441998264\n\n\n\nsent.similarity(nlp('Haec narrant poetae de Perseo.'))\n\n0.7961655556379285\n\n\nWe can retrieve the start and end indices from the original document for each sentence.\n\nsent_2 = sents[1]\nstart = sent_2.start\nend = sent_2.end\nprint(start)\nprint(end)\nprint(sent_2.text)\nprint(doc[start:end].text)\n\n7\n15\nPerseus filius erat Iovis, maximi deorum.\nPerseus filius erat Iovis, maximi deorum.\n\n\n\nReferences\nSLP Chapter 2, Section 2.4.5 ‚ÄúSentence Segmentation‚Äù, pp.¬†24 link\n\n\n\n\nClayman, Dee. 1981. ‚ÄúSentence Length in Greek Hexameter Poetry.‚Äù Quantitative Linguistics 11: 107‚Äì36. https://papers.ssrn.com/abstract=1627358.\n\n\nJanson, Tore. 1964. ‚ÄúThe Problems of Measuring Sentence-Length in Classical Texts.‚Äù Studia Linguistica 18 (1): 26‚Äì36. https://doi.org/10.1111/j.1467-9582.1964.tb00443.x.\n\n\nWake, William C. 1957. ‚ÄúSentence-Length Distributions of Greek Authors.‚Äù Journal of the Royal Statistical Society. Series A (General) 120 (3): 331‚Äì46. https://www.jstor.org/stable/2343104."
  },
  {
    "objectID": "word-tokenization.html#word-tokenization-with-latincy",
    "href": "word-tokenization.html#word-tokenization-with-latincy",
    "title": "4¬† Word Tokenization",
    "section": "4.1 Word tokenization with LatinCy",
    "text": "4.1 Word tokenization with LatinCy\n\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_md')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\n\n\nWord tokenization is the task of splitting a text into words (and wordlike units like punctuation, numbers, etc.). For the LatinCy models, tokenization is the fundamental pipeline component on which all other components depend. SpaCy uses non-destructive, ‚Äúcanonical‚Äù tokenization, i.e.¬†non-destructive, in that the original text can be untokenized, so to speak, based on Token annotations and canonical in that indices are assigned to each token during this process and these indices are used to refer to the tokens in other annotations. (Tokens can be separated or merged, but this requires the user to actively undo and redefine the tokenization output.) LatinCy uses a modified version of the default spaCy tokenizer that recognizes a splits enlitic -que using a rules-based process. (NB: It is in the LatinCy development plan to move enclitic splitting to a separate post-tokenization component.)\nThe spaCy Doc object is an iterable and tokens are the iteration unit.\n\ntokens = [item for item in doc]\nprint(tokens)\n\n[Haec, narrantur, a, poetis, de, Perseo, ., Perseus, filius, erat, Iovis, ,, maximi, deorum, ., Avus, eius, Acrisius, appellabatur, .]\n\n\n\ntoken = tokens[0]\nprint(type(token))\n\n&lt;class 'spacy.tokens.token.Token'&gt;\n\n\nThe text content of a Token object can be retrieved with the text attribute.\n\nfor i, token in enumerate(tokens, 1):\n    print(f'{i}: {token.text}')\n\n1: Haec\n2: narrantur\n3: a\n4: poetis\n5: de\n6: Perseo\n7: .\n8: Perseus\n9: filius\n10: erat\n11: Iovis\n12: ,\n13: maximi\n14: deorum\n15: .\n16: Avus\n17: eius\n18: Acrisius\n19: appellabatur\n20: .\n\n\nNote again that the token itself is a spaCy Token object and that the text attribute returns a Python string even though their representations in the Jupyter Notebook look the same.\n\ntoken = tokens[0]\nprint(f'{type(token)} -&gt; {token}')\nprint(f'{type(token.text)} -&gt; {token.text}')\n\n&lt;class 'spacy.tokens.token.Token'&gt; -&gt; Haec\n&lt;class 'str'&gt; -&gt; Haec\n\n\n\n4.1.1 Token attributes and methods related to tokenization\nHere are some atrributes/methods available for spaCy Token objects that are relevant to word tokenization.\nSpaCy keeps track of both the token indices and the character offsets within a doc using either the i or idx attributes, respectively‚Ä¶\n\nprint(token.doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\n\n\n\n# token indices\n\nfor token in doc:\n    print(f'{token.i}: {token.text}')\n\n0: Haec\n1: narrantur\n2: a\n3: poetis\n4: de\n5: Perseo\n6: .\n7: Perseus\n8: filius\n9: erat\n10: Iovis\n11: ,\n12: maximi\n13: deorum\n14: .\n15: Avus\n16: eius\n17: Acrisius\n18: appellabatur\n19: .\n\n\nThis is functionally equivalent to using enumerate‚Ä¶\n\n# token indices, with enumerate\n\nfor i, token in enumerate(doc):\n    print(f'{i}: {token.text}')\n\n0: Haec\n1: narrantur\n2: a\n3: poetis\n4: de\n5: Perseo\n6: .\n7: Perseus\n8: filius\n9: erat\n10: Iovis\n11: ,\n12: maximi\n13: deorum\n14: .\n15: Avus\n16: eius\n17: Acrisius\n18: appellabatur\n19: .\n\n\nAnother indexing option is the idx attribute which is the character offset of the token in the original Doc object.\n\n# character offsets, \nfor token in doc:\n    print(f'{token.idx}: {token.text}')\n\n0: Haec\n5: narrantur\n15: a\n17: poetis\n24: de\n27: Perseo\n33: .\n35: Perseus\n43: filius\n50: erat\n55: Iovis\n60: ,\n62: maximi\n69: deorum\n75: .\n77: Avus\n82: eius\n87: Acrisius\n96: appellabatur\n108: .\n\n\nObserve these idx attributes relate to the character offsets from the original Doc. To illustrate the point, we will replace spaces with an underscore in the output. We can see from the output above that narrantur begins at idx 5 and that the next word a begins at idx 15. Yet narrantur is only 9 characters long and the difference between these two numbers is 10! This is because we need to account for whitespace in the original Doc. This is handled by the attribute text_with_ws.\n\nprint(doc.text[5:15].replace(' ', '_'))\n\nnarrantur_\n\n\n\n\nprint(f'text -&gt; {doc[1].text} (length {len(doc[1].text)})')\nprint()\nprint(f'text_with_ws -&gt; {doc[1].text_with_ws} (length {len(doc[1].text_with_ws)})')\n\ntext -&gt; narrantur (length 9)\n\ntext_with_ws -&gt; narrantur  (length 10)\n\n\nAccordingly, using the text_with_ws attribute (as opposed to simply the text attribute) we can reconstruct the original text. This is what was meant above by ‚Äúnon-destructive‚Äù tokenization. Look at the difference between a text joined using the text attribute and one joined using the text_with_ws attribute.\n\njoined_tokens = ' '.join([token.text for token in doc])\nprint(joined_tokens)\nprint(joined_tokens == doc.text)\n\nprint()\n\nreconstructed_text = ''.join([token.text_with_ws for token in doc])\nprint(reconstructed_text)\nprint(reconstructed_text == doc.text)\n\nHaec narrantur a poetis de Perseo . Perseus filius erat Iovis , maximi deorum . Avus eius Acrisius appellabatur .\nFalse\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nTrue\n\n\nBecause spaCy tokenization is set from the outset, you can traverse the tokens in a Doc objects from the tokens themselves using the nbor method. This method takes an integer argument that specifies the number of tokens to traverse. A positive integer traverses the tokens to the right, a negative integer traverses the tokens to the left.\n\nprint(doc[:6])\nprint('-----')\nprint(f'{doc[3]}, i.e. i = 3')\nprint(f'{doc[3].nbor(-1)}, i.e. i - 1 = 2')\nprint(f'{doc[3].nbor(-2)}, i.e. i - 2 = 1')\nprint(f'{doc[3].nbor(1)}, i.e. i + 1 = 4')\nprint(f'{doc[3].nbor(2)}, i.e. i + 2 = 5')\n\nHaec narrantur a poetis de Perseo\n-----\npoetis, i.e. i = 3\na, i.e. i - 1 = 2\nnarrantur, i.e. i - 2 = 1\nde, i.e. i + 1 = 4\nPerseo, i.e. i + 2 = 5\n\n\n\n\n4.1.2 Customization of the spaCy tokenizer in LatinCy\nLatinCy aims to tokenize the que enclitic in Latin texts. As noted above this is currently done through a rule-based approach. Here is the custom tokenizer code (beginning at this line in the code) followed by a description of the process. Note that this process is based on the following recommendations in the spaCy documentation: https://spacy.io/usage/training#custom-tokenizer.\n\nfrom spacy.util import registry, compile_suffix_regex\n\n@registry.callbacks(\"customize_tokenizer\")\ndef make_customize_tokenizer():\n    def customize_tokenizer(nlp):\n        suffixes = nlp.Defaults.suffixes + [\n            \"que\",\n            \"qve\",\n        ]\n        suffix_regex = compile_suffix_regex(suffixes)\n        nlp.tokenizer.suffix_search = suffix_regex.search\n\n        for item in que_exceptions:\n            nlp.tokenizer.add_special_case(item, [{\"ORTH\": item}])\n            nlp.tokenizer.add_special_case(item.lower(), [{\"ORTH\": item.lower()}])\n            nlp.tokenizer.add_special_case(item.title(), [{\"ORTH\": item.title()}])\n            nlp.tokenizer.add_special_case(item.upper(), [{\"ORTH\": item.upper()}])\n\n    return customize_tokenizer\n\nBasically, we treat que (and its case and u/v norm variants) as punctuation. These are added to the Defaults.suffixes. If no other intervention were made, then any word ending in que or a variant would be split into a before-que part and que. Since there are large number of relatively predictable words that end in que, these are maintained in a list called que_exceptions. All of the words in the que_exceptions list are added as a ‚Äúspecial case‚Äù using the tokenizer‚Äôs add_special_case method and so will not be split. The que_exceptions lists is as follows:\n\nque_exceptions = ['quisque', 'quidque', 'quicque', 'quodque', 'cuiusque', 'cuique', 'quemque', 'quamque', 'quoque', 'quaque', 'quique', 'quaeque', 'quorumque', 'quarumque', 'quibusque', 'quosque', 'quasque', 'uterque', 'utraque', 'utrumque', 'utriusque', 'utrique', 'utrumque', 'utramque', 'utroque', 'utraque', 'utrique', 'utraeque', 'utrorumque', 'utrarumque', 'utrisque', 'utrosque', 'utrasque', 'quicumque', 'quidcumque', 'quodcumque', 'cuiuscumque', 'cuicumque', 'quemcumque', 'quamcumque', 'quocumque', 'quacumque', 'quicumque', 'quaecumque', 'quorumcumque', 'quarumcumque', 'quibuscumque', 'quoscumque', 'quascumque', 'unusquisque', 'unaquaeque', 'unumquodque', 'unumquidque', 'uniuscuiusque', 'unicuique', 'unumquemque', 'unamquamque', 'unoquoque', 'unaquaque', 'plerusque', 'pleraque', 'plerumque', 'plerique', 'pleraeque', 'pleroque', 'pleramque', 'plerorumque', 'plerarumque', 'plerisque', 'plerosque', 'plerasque', 'absque', 'abusque', 'adaeque', 'adusque', 'aeque', 'antique', 'atque', 'circumundique', 'conseque', 'cumque', 'cunque', 'denique', 'deque', 'donique', 'hucusque', 'inique', 'inseque', 'itaque', 'longinque', 'namque', 'neque', 'oblique', 'peraeque', 'praecoque', 'propinque', 'qualiscumque', 'quandocumque', 'quandoque', 'quantuluscumque', 'quantumcumque', 'quantuscumque', 'quinque', 'quocumque', 'quomodocumque', 'quomque', 'quotacumque', 'quotcumque', 'quotienscumque', 'quotiensque', 'quotusquisque', 'quousque', 'relinque', 'simulatque', 'torque', 'ubicumque', 'ubique', 'undecumque', 'undique', 'usque', 'usquequaque', 'utcumque', 'utercumque', 'utique', 'utrimque', 'utrique', 'utriusque', 'utrobique', 'utrubique']\n\nYou can see these words in the rules attribute of the tokenizer.\n\n# Sample of 10 que rules from the custom tokenizer\n\ntokenizer_rules = nlp.tokenizer.rules\nprint(sorted(list(set([rule.lower() for rule in tokenizer_rules if 'que' in rule])))[:10])\n\n['absque', 'abusque', 'adaeque', 'adusque', 'aeque', 'antique', 'atque', 'circumundique', 'conseque', 'cuicumque']\n\n\nWith the exception of basic enclitic splitting, the LatinCy tokenizer is the same as the default spaCy tokenizer. The default spaCy tokenizer is described in detail in the spaCy documentation. Here are some useful attributes/methods for working with LatinCy.\nTokenize a string without any other pipeline annotations with a call.\n\ntokens = nlp.tokenizer(text)\nprint(tokens)\nprint(tokens[0].text)\nprint(tokens[0].lemma_) # Note that there is no annotation here because since the tokenizer has been called directly, the lemmatizer‚Äîthe entire pipeline, in fact‚Äîhas not been run\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nHaec\n\n\n\nA list of texts can be tokenized in one pass with the pipe method. This yields a generator object where each item is Doc object of tokenized-only texts\n\ntexts = [\"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum; avus eius Acrisius appellabatur. Acrisius volebat Perseum nepotem suum necare; nam propter oraculum puerum timebat. Comprehendit igitur Perseum adhuc infantem, et cum matre in arca lignea inclusit. Tum arcam ipsam in mare coniecit. Danae, Persei mater, magnopere territa est; tempestas enim magna mare turbabat. Perseus autem in sinu matris dormiebat.\", \"Iuppiter tamen haec omnia vidit, et filium suum servare constituit. Tranquillum igitur fecit mare, et arcam ad insulam Seriphum perduxit. Huius insulae Polydectes tum rex erat. Postquam arca ad litus appulsa est, Danae in harena quietem capiebat. Post breve tempus a piscatore quodam reperta est, et ad domum regis Polydectis adducta est. Ille matrem et puerum benigne excepit, et iis sedem tutam in finibus suis dedit. Danae hoc donum libenter accepit, et pro tanto beneficio regi gratias egit.\", \"Perseus igitur multos annos ibi habitabat, et cum matre sua vitam beatam agebat. At Polydectes Danaen magnopere amabat, atque eam in matrimonium ducere volebat. Hoc tamen consilium Perseo minime gratum erat. Polydectes igitur Perseum dimittere constituit. Tum iuvenem ad se vocavit et haec dixit: \\\"Turpe est hanc ignavam vitam agere; iam dudum tu adulescens es. Quo usque hic manebis? Tempus est arma capere et virtutem praestare. Hinc abi, et caput Medusae mihi refer.\\\"\"]\n\ntokens = list(nlp.tokenizer.pipe(texts))\n\nprint(len(tokens)) # number of documents\nprint(len(tokens[0])) # number of tokens in first document\n\n3\n76\n\n\nYou can get an explanation of the tokenization ‚Äúdecisions‚Äù using the explain method. In the example below, we see how the que in virumque is treated as a suffix (as discussed above) and so is split during tokenization.\n\ntok_exp = nlp.tokenizer.explain('arma virumque cano')\nprint(tok_exp)\n\n[('TOKEN', 'arma'), ('TOKEN', 'virum'), ('SUFFIX', 'que'), ('TOKEN', 'cano')]\n\n\n\ntokens = nlp.tokenizer('arma uirumque cano')\nfor i, token in enumerate(tokens):\n    print(f'{i}: {token.text}')\n\n0: arma\n1: uirum\n2: que\n3: cano\n\n\n\n\nReferences\nSLP Chapter 2, Section 2.4.2 ‚ÄúWord Tokenization‚Äù, pp.¬†18-20 link"
  },
  {
    "objectID": "projects.html#enclitics-project",
    "href": "projects.html#enclitics-project",
    "title": "Open LatinCy Projects",
    "section": "Model-based enclitic splitting",
    "text": "Model-based enclitic splitting\nAs discussed in the Word Tokenization chapter, enclitic splitting is currently limited to que (and variants) and is a rules-based process that is part of the LatinCy custom tokenizer. It would be preferable to make enclitic splitting a separate pipeline component and moreover one that is model-based. This component could be placed immediately after the tokenizer and use the Retokenizer.split method to reconstruct token sequences where valid enclitics are identified. This would have the added advantage of allowing enclitic splitting to be ‚Äúturned off‚Äù, so to speak, by removing the component from the pipeline."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Clayman, Dee. 1981. ‚ÄúSentence Length in\nGreek Hexameter Poetry.‚Äù\nQuantitative Linguistics 11: 107‚Äì36. https://papers.ssrn.com/abstract=1627358.\n\n\nJanson, Tore. 1964. ‚ÄúThe Problems of\nMeasuring Sentence-Length in\nClassical Texts.‚Äù Studia\nLinguistica 18 (1): 26‚Äì36. https://doi.org/10.1111/j.1467-9582.1964.tb00443.x.\n\n\nJurafsky, Daniel, and James H. Martin. 2020. ‚ÄúSpeech and\nLanguage Processing (3rd Edition,\nDraft).‚Äù https://web.stanford.edu/~jurafsky/slp3/.\n\n\nWake, William C. 1957. ‚ÄúSentence-Length Distributions of\nGreek Authors.‚Äù Journal of the Royal Statistical\nSociety. Series A (General) 120 (3): 331‚Äì46. https://www.jstor.org/stable/2343104."
  }
]